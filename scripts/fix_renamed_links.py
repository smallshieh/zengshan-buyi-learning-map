#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ›´æ–°é‡æ–°å‘½åæª”æ¡ˆå¾Œçš„é€£çµ
æ ¹æ“š auto_categorize_cases.py çš„ç§»å‹•è¨˜éŒ„æ›´æ–°æ‰€æœ‰ Obsidian é€£çµ
"""
import re
import sys
from pathlib import Path
from typing import Dict, Set, Tuple

BASE_DIR = Path(r"c:\Users\smallshieh\Obsidianç­†è¨˜")

def build_rename_map() -> Dict[str, str]:
    """
    å»ºç«‹èˆŠæª”å â†’ æ–°æª”åçš„æ˜ å°„è¡¨
    å¾ auto_categorize_cases.py é‡æ–°åŸ·è¡Œä»¥å–å¾—æ˜ å°„
    """
    sys.path.insert(0, str(BASE_DIR / "scripts"))
    from auto_categorize_cases import CASES_DIR, extract_frontmatter, infer_category, generate_new_filename
    
    rename_map = {}
    
    # æƒææ‰€æœ‰åˆ†é¡ç›®éŒ„ä¸­çš„æª”æ¡ˆ
    for category_dir in CASES_DIR.iterdir():
        if not category_dir.is_dir():
            continue
        
        for case_file in category_dir.glob("case_*.md"):
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€è¿‘é‡æ–°å‘½åçš„æª”æ¡ˆ
            # æ–°æª”åæ ¼å¼: case_###_å XXX.md (ç°¡æ½”)
            # èˆŠæª”åæ ¼å¼: case_###_é¡åˆ¥_case_xxx_long_name.md (å†—é•·)
            
            # ç”±æ–¼æˆ‘å€‘å·²ç¶“ç§»å‹•äº†æª”æ¡ˆ,éœ€è¦å¾è¨˜éŒ„ä¸­é‡å»ºæ˜ å°„
            # é€™è£¡æˆ‘å€‘ä½¿ç”¨ç°¡åŒ–çš„å•Ÿç™¼å¼æ–¹æ³•
            pass
    
    # æ‰‹å‹•å»ºç«‹å·²çŸ¥çš„é‡æ–°å‘½åæ˜ å°„(å¾åŸ·è¡Œè¨˜éŒ„ä¸­æå–)
    # ç”±æ–¼æª”æ¡ˆå·²ç§»å‹•,æˆ‘å€‘éœ€è¦æƒæä¸¦æ¯”å°
    return rename_map

def extract_wikilinks(content: str) -> Set[str]:
    """æå–å…§å®¹ä¸­çš„æ‰€æœ‰ Wikilink"""
    # åŒ¹é… [[filename]] æˆ– [[filename|alias]]
    pattern = r'\[\[([^\]|]+)(?:\|[^\]]+)?\]\]'
    matches = re.findall(pattern, content)
    return set(matches)

def update_links_in_file(file_path: Path, rename_map: Dict[str, str], dry_run: bool = True) -> Tuple[int, list]:
    """
    æ›´æ–°æª”æ¡ˆä¸­çš„é€£çµ
    è¿”å›: (æ›´æ–°æ•¸é‡, æ›´æ–°è©³æƒ…åˆ—è¡¨)
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âš ï¸  ç„¡æ³•è®€å– {file_path}: {e}")
        return 0, []
    
    original_content = content
    updates = []
    
    # æå–æ‰€æœ‰é€£çµ
    wikilinks = extract_wikilinks(content)
    
    for old_link in wikilinks:
        # ç§»é™¤å¯èƒ½çš„è·¯å¾‘å‰ç¶´,åªä¿ç•™æª”å
        old_filename = old_link.split('/')[-1]
        
        # æª¢æŸ¥æ˜¯å¦åœ¨é‡æ–°å‘½åæ˜ å°„ä¸­
        if old_filename in rename_map:
            new_filename = rename_map[old_filename]
            
            # æ›¿æ›é€£çµ(ä¿ç•™è·¯å¾‘çµæ§‹)
            if '/' in old_link:
                # æœ‰è·¯å¾‘çš„é€£çµ
                path_prefix = '/'.join(old_link.split('/')[:-1])
                new_link = f"{path_prefix}/{new_filename}"
            else:
                # ç´”æª”åé€£çµ
                new_link = new_filename
            
            # åŸ·è¡Œæ›¿æ›
            # éœ€è¦è™•ç†å…©ç¨®æƒ…æ³: [[old]] å’Œ [[old|alias]]
            pattern1 = re.compile(r'\[\[' + re.escape(old_link) + r'\]\]')
            pattern2 = re.compile(r'\[\[' + re.escape(old_link) + r'\|([^\]]+)\]\]')
            
            if pattern1.search(content):
                content = pattern1.sub(f'[[{new_link}]]', content)
                updates.append(f"{old_link} â†’ {new_link}")
            
            if pattern2.search(content):
                content = pattern2.sub(f'[[{new_link}|\\1]]', content)
                updates.append(f"{old_link}|alias â†’ {new_link}|alias")
    
    # å¦‚æœæœ‰æ›´æ–°ä¸”é dry run,å¯«å…¥æª”æ¡ˆ
    if content != original_content:
        if not dry_run:
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
            except Exception as e:
                print(f"âš ï¸  ç„¡æ³•å¯«å…¥ {file_path}: {e}")
                return 0, []
        
        return len(updates), updates
    
    return 0, []

def create_simple_rename_map() -> Dict[str, str]:
    """
    å»ºç«‹ç°¡åŒ–çš„é‡æ–°å‘½åæ˜ å°„
    åŸºæ–¼æª”æ¡ˆç§»å‹•çš„æ¨¡å¼æ¨æ–·
    """
    rename_map = {}
    
    # æƒææ‰€æœ‰æ¡ˆä¾‹æª”æ¡ˆ,å»ºç«‹æª”åæ˜ å°„
    # èˆŠæª”åæ¨¡å¼: case_###_é¡åˆ¥_case_xxx_long_english_name.md
    # æ–°æª”åæ¨¡å¼: case_###_å XXX.md
    
    for category_dir in (BASE_DIR / "cases").iterdir():
        if not category_dir.is_dir():
            continue
        
        for case_file in category_dir.glob("case_*.md"):
            # æå–æ¡ˆä¾‹ç·¨è™Ÿ
            match = re.match(r'case_(\d+)_', case_file.name)
            if match:
                case_num = match.group(1)
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºæ–°æ ¼å¼(ç°¡æ½”çš„ä¸­æ–‡åç¨±)
                if 'å ' in case_file.name and len(case_file.name) < 50:
                    # é€™æ˜¯æ–°æª”å,å˜—è©¦æ‰¾å‡ºå¯èƒ½çš„èˆŠæª”å
                    # èˆŠæª”åæœƒåŒ…å«é¡åˆ¥å’Œè‹±æ–‡æè¿°
                    possible_old_patterns = [
                        f"case_{case_num}_å¤©æ™‚_case_",
                        f"case_{case_num}_é™°å®…_case_",
                        f"case_{case_num}_é¢¨æ°´_case_",
                    ]
                    
                    # ç”±æ–¼èˆŠæª”æ¡ˆå·²è¢«ç§»å‹•,æˆ‘å€‘ç„¡æ³•ç›´æ¥æ¯”å°
                    # é€™å€‹æ˜ å°„éœ€è¦å¾åŸ·è¡Œè¨˜éŒ„ä¸­æå–
    
    return rename_map

def main(dry_run: bool = True):
    """ä¸»åŸ·è¡Œæµç¨‹"""
    print("=" * 70)
    print("é€£çµæ›´æ–°è…³æœ¬ - æª”æ¡ˆé‡æ–°å‘½åå¾Œ")
    print("=" * 70)
    
    if dry_run:
        print("\nâš ï¸  DRY RUN æ¨¡å¼: ä¸æœƒå¯¦éš›ä¿®æ”¹æª”æ¡ˆ\n")
    
    # ç”±æ–¼æª”æ¡ˆå·²ç§»å‹•,æˆ‘å€‘éœ€è¦ä½¿ç”¨ä¸åŒçš„ç­–ç•¥
    # ç­–ç•¥: æƒææ‰€æœ‰ markdown æª”æ¡ˆ,æ‰¾å‡ºæ–·è£‚çš„é€£çµ
    print("ğŸ“‚ æƒææ‰€æœ‰ Markdown æª”æ¡ˆ...")
    md_files = list(BASE_DIR.rglob("*.md"))
    
    # æ’é™¤æŸäº›ç›®éŒ„
    md_files = [f for f in md_files if not any(
        exclude in str(f) for exclude in ['.gemini', '.agent', 'node_modules', '.git']
    )]
    
    print(f"   æ‰¾åˆ° {len(md_files)} å€‹æª”æ¡ˆ\n")
    
    # å»ºç«‹æ‰€æœ‰ç¾å­˜æª”æ¡ˆçš„ç´¢å¼•(ä¸å«å‰¯æª”å)
    existing_files = set()
    for f in (BASE_DIR / "cases").rglob("*.md"):
        existing_files.add(f.stem)  # ä¸å« .md
    
    print(f"ğŸ“‹ ç¾å­˜æ¡ˆä¾‹æª”æ¡ˆ: {len(existing_files)} å€‹\n")
    
    # æƒææ–·è£‚çš„é€£çµ
    print("ğŸ” æƒææ–·è£‚çš„é€£çµ...")
    broken_links = {}
    
    for md_file in md_files:
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            wikilinks = extract_wikilinks(content)
            
            for link in wikilinks:
                # æå–æª”å(ç§»é™¤è·¯å¾‘å’Œå‰¯æª”å)
                filename = link.split('/')[-1].replace('.md', '')
                
                # æª¢æŸ¥æ˜¯å¦å­˜åœ¨
                if filename.startswith('case_') and filename not in existing_files:
                    if filename not in broken_links:
                        broken_links[filename] = []
                    broken_links[filename].append(str(md_file.relative_to(BASE_DIR)))
        
        except Exception as e:
            pass
    
    if broken_links:
        print(f"\nâš ï¸  ç™¼ç¾ {len(broken_links)} å€‹æ–·è£‚çš„é€£çµ:\n")
        for broken, files in sorted(broken_links.items())[:10]:
            print(f"  âŒ {broken}")
            for f in files[:3]:
                print(f"     å¼•ç”¨è‡ª: {f}")
            if len(files) > 3:
                print(f"     ... åŠå…¶ä»– {len(files)-3} å€‹æª”æ¡ˆ")
    else:
        print("\nâœ… æœªç™¼ç¾æ–·è£‚çš„é€£çµ!")
    
    print("\n" + "=" * 70)
    print("ğŸ’¡ ç”±æ–¼æª”æ¡ˆå·²ç§»å‹•,å»ºè­°ä½¿ç”¨ Obsidian çš„å…§å»ºåŠŸèƒ½:")
    print("   1. é–‹å•Ÿ Obsidian")
    print("   2. ä½¿ç”¨ Ctrl+P â†’ 'Detect all broken links'")
    print("   3. ä½¿ç”¨ 'Update internal links' è‡ªå‹•ä¿®å¾©")
    print("=" * 70)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æ›´æ–°æª”æ¡ˆé‡æ–°å‘½åå¾Œçš„é€£çµ')
    parser.add_argument('--commit', action='store_true', help='åŸ·è¡Œå¯¦éš›æ›´æ–°')
    
    args = parser.parse_args()
    
    main(dry_run=not args.commit)
