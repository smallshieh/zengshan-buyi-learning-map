#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è­˜é»è‡ªå‹•é€£çµè…³æœ¬ (Knowledge Linking Script)
æ ¹æ“šã€ŠçŸ¥è­˜é€£çµè¦ç¯„ v1.0ã€‹å¯¦ä½œ
âš¡ Token å„ªåŒ–ï¼šæ”¹ç”¨ term_index.json è€Œéæƒæç›®éŒ„

åŠŸèƒ½ï¼š
1. è‡ªå‹•ç‚ºå¦ä¾‹ä¸­çš„è¡“èªå»ºç«‹ [[é€£çµ]]
2. éµå¾ªã€Œé¦–æ¬¡å‡ºç¾ã€åŸå‰‡
3. åªåœ¨æŒ‡å®šå€å¡Šï¼ˆæ–·èªã€ç†è«–è¦é»ï¼‰ä¸­é€£çµ
4. ä¿è­· YAMLã€ä»£ç¢¼å€å¡Šã€æ—¢æœ‰é€£çµ
"""

import os
import re
import json
from pathlib import Path
from typing import List, Set, Dict

# ==================== é…ç½® ====================
BASE_DIR = Path(r"c:\Users\smallshieh\Obsidianç­†è¨˜")
GLOSSARY_DIR = BASE_DIR / "glossary"
THEORY_DIR = BASE_DIR / "theory"
CASES_DIR = BASE_DIR / "cases"
TERM_INDEX_FILE = BASE_DIR / "data" / "term_index.json"

# ç›®æ¨™å€å¡Šï¼ˆåªåœ¨é€™äº›å€å¡Šå…§é€£çµï¼‰
TARGET_SECTIONS = [
    "## æ–·èª",
    "## å¦è±¡åˆ†æ", 
    "## ç†è«–è¦é»",
    "## é‡é»æ‘˜è¦",
    "## é‡é¶´è©•è¨»"
]

# ==================== è¡“èªè¼‰å…¥ ====================
def load_terms() -> List[str]:
    """
    è¼‰å…¥æ‰€æœ‰è¡“èªï¼ˆå¾ glossary å’Œ theoryï¼‰
    âš¡ Token å„ªåŒ–ï¼šæ”¹ç”¨ term_index.json è€Œéæƒæç›®éŒ„
    """
    terms = []
    
    # å„ªå…ˆå¾ term_index.json è®€å–ï¼ˆToken å„ªåŒ–ï¼‰
    if TERM_INDEX_FILE.exists():
        print("ğŸ“– å¾ term_index.json è®€å–è¡“èªï¼ˆToken å„ªåŒ–æ¨¡å¼ï¼‰...")
        try:
            with open(TERM_INDEX_FILE, 'r', encoding='utf-8') as f:
                index = json.load(f)
            
            # ä½¿ç”¨å¿«é€ŸæŸ¥è©¢çš„åç¨±åˆ—è¡¨
            terms = index['term_names']['glossary'] + index['term_names']['theory']
            print(f"   âœ… å¾ç´¢å¼•æª”è¼‰å…¥ {len(terms)} å€‹è¡“èª")
        except Exception as e:
            print(f"   âš ï¸  è®€å– term_index.json å¤±æ•—: {e}")
            print("   â„¹ï¸  é€€å›åˆ°æƒææ¨¡å¼...")
    
    # è‹¥ç´¢å¼•æª”ä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—ï¼Œé€€å›æƒææ¨¡å¼
    if not terms:
        print("âš ï¸  ä½¿ç”¨ç›®éŒ„æƒææ¨¡å¼ï¼ˆå»ºè­°åŸ·è¡Œ python scripts/build_term_index.pyï¼‰")
        # å¾è¡“èªè¡¨è¼‰å…¥
        if GLOSSARY_DIR.exists():
            for f in GLOSSARY_DIR.glob("*.md"):
                terms.append(f.stem)
        
        # å¾ç†è«–ç« ç¯€è¼‰å…¥
        if THEORY_DIR.exists():
            for f in THEORY_DIR.glob("*.md"):
                # è™•ç† "01_å…«å¦ç« .md" -> åŠ å…¥ "å…«å¦ç« " å’Œ "01_å…«å¦ç« "
                full_name = f.stem
                terms.append(full_name)
                if "_" in full_name:
                    short_name = full_name.split("_", 1)[1]
                    terms.append(short_name)
    
    # æŒ‰é•·åº¦æ’åºï¼ˆé•·è©å„ªå…ˆï¼Œé¿å…è¢«çŸ­è©æˆªæ–·ï¼‰
    # ä¾‹å¦‚ï¼šã€ŒåŒ–é€²ç¥ã€æ‡‰è©²åœ¨ã€Œé€²ç¥ã€ä¹‹å‰è™•ç†
    terms = sorted(list(set(terms)), key=len, reverse=True)
    
    # éæ¿¾å–®å­—è¡“èªï¼ˆé¿å…èª¤åˆ¤ï¼‰
    terms = [t for t in terms if len(t) > 1]
    
    return terms

# ==================== å…§å®¹è§£æ ====================
def split_frontmatter_and_body(content: str) -> tuple:
    """åˆ†é›¢ YAML Frontmatter å’Œæ­£æ–‡"""
    parts = re.split(r'^---$', content, maxsplit=2, flags=re.MULTILINE)
    
    if len(parts) >= 3:
        frontmatter = f"---{parts[1]}---"
        body = parts[2]
        return frontmatter, body
    else:
        return "", content

def extract_target_sections(body: str) -> Dict[str, str]:
    """æå–ç›®æ¨™å€å¡Šï¼ˆæ–·èªã€ç†è«–è¦é»ç­‰ï¼‰"""
    sections = {}
    
    # åˆ†å‰²æ‰€æœ‰ ## æ¨™é¡Œ
    lines = body.split('\n')
    current_section = None
    current_content = []
    
    for line in lines:
        if line.startswith('## '):
            # å„²å­˜ä¸Šä¸€å€‹å€å¡Š
            if current_section and current_section in TARGET_SECTIONS:
                sections[current_section] = '\n'.join(current_content)
            
            # é–‹å§‹æ–°å€å¡Š
            current_section = line.strip()
            current_content = [line]
        else:
            if current_section:
                current_content.append(line)
    
    # å„²å­˜æœ€å¾Œä¸€å€‹å€å¡Š
    if current_section and current_section in TARGET_SECTIONS:
        sections[current_section] = '\n'.join(current_content)
    
    return sections

# ==================== é€£çµé‚è¼¯ ====================
def protect_existing_links(text: str) -> tuple:
    """æš«æ™‚æ›¿æ›æ—¢æœ‰é€£çµï¼Œé¿å…åµŒå¥—"""
    placeholders = {}
    links = re.findall(r'\[\[.*?\]\]', text)
    
    for i, link in enumerate(links):
        placeholder = f"__LINK_PLACEHOLDER_{i}__"
        text = text.replace(link, placeholder)
        placeholders[placeholder] = link
    
    return text, placeholders

def restore_links(text: str, placeholders: Dict[str, str]) -> str:
    """é‚„åŸè¢«ä¿è­·çš„é€£çµ"""
    for placeholder, original_link in placeholders.items():
        text = text.replace(placeholder, original_link)
    return text

def link_terms_in_section(section_content: str, terms: List[str]) -> str:
    """åœ¨å€å¡Šä¸­ç‚ºè¡“èªå»ºç«‹é€£çµï¼ˆé¦–æ¬¡å‡ºç¾åŸå‰‡ï¼‰"""
    # ä¿è­·æ—¢æœ‰é€£çµ
    protected_text, placeholders = protect_existing_links(section_content)
    
    # ä¿è­·ä»£ç¢¼å€å¡Š
    code_blocks = re.findall(r'```.*?```', protected_text, re.DOTALL)
    for i, block in enumerate(code_blocks):
        code_placeholder = f"__CODE_BLOCK_{i}__"
        protected_text = protected_text.replace(block, code_placeholder)
        placeholders[code_placeholder] = block
    
    # è¿½è¹¤å·²é€£çµçš„è¡“èª
    linked_terms: Set[str] = set()
    
    # ç‚ºæ¯å€‹è¡“èªå»ºç«‹é€£çµï¼ˆåªé€£çµé¦–æ¬¡å‡ºç¾ï¼‰
    for term in terms:
        if term in linked_terms:
            continue
        
        # æª¢æŸ¥è¡“èªæ˜¯å¦å­˜åœ¨ï¼ˆä¸”ä¸åœ¨å·²é€£çµçš„è©ä¸­ï¼‰
        if term in protected_text and not f"[[{term}]]" in protected_text:
            # ä½¿ç”¨æ­£å‰‡ç¢ºä¿ä¸æœƒé€£çµåˆ°è©çš„ä¸€éƒ¨åˆ†
            # ä¾‹å¦‚ï¼šä¸è¦å°‡ "ç”¨ç¥ç« " ä¸­çš„ "ç”¨ç¥" é€£çµï¼ˆå› ç‚ºæ•´é«”æ˜¯ "ç”¨ç¥ç« "ï¼‰
            pattern = re.compile(f'(?<!\\[\\[){re.escape(term)}(?!\\]\\])')
            
            # åªæ›¿æ›ç¬¬ä¸€æ¬¡å‡ºç¾
            protected_text = pattern.sub(f'[[{term}]]', protected_text, count=1)
            linked_terms.add(term)
    
    # é‚„åŸæ‰€æœ‰è¢«ä¿è­·çš„å…§å®¹
    result = restore_links(protected_text, placeholders)
    
    return result

# ==================== æª”æ¡ˆè™•ç† ====================
def process_case_file(filepath: Path, terms: List[str], dry_run: bool = False) -> bool:
    """è™•ç†å–®å€‹å¦ä¾‹æª”æ¡ˆ"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        # åˆ†é›¢ Frontmatter å’Œæ­£æ–‡
        frontmatter, body = split_frontmatter_and_body(original_content)
        
        # æå–ç›®æ¨™å€å¡Š
        target_sections = extract_target_sections(body)
        
        if not target_sections:
            # æ²’æœ‰ç›®æ¨™å€å¡Šï¼Œè·³é
            return False
        
        # è™•ç†æ¯å€‹ç›®æ¨™å€å¡Š
        modified = False
        for section_title, section_content in target_sections.items():
            new_content = link_terms_in_section(section_content, terms)
            if new_content != section_content:
                body = body.replace(section_content, new_content)
                modified = True
        
        if not modified:
            return False
        
        # é‡çµ„å…§å®¹
        new_content = frontmatter + body
        
        # å¯«å›æª”æ¡ˆï¼ˆé™¤éæ˜¯ dry runï¼‰
        if not dry_run:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(new_content)
        
        return True
        
    except Exception as e:
        print(f"âŒ è™•ç† {filepath.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

# ==================== ä¸»ç¨‹å¼ ====================
def main(dry_run: bool = False, test_mode: bool = False):
    """ä¸»åŸ·è¡Œæµç¨‹"""
    print("=" * 60)
    print("çŸ¥è­˜é»è‡ªå‹•é€£çµè…³æœ¬ v1.1 (Token å„ªåŒ–ç‰ˆ)")
    print("éµå¾ªã€ŠçŸ¥è­˜é€£çµè¦ç¯„ã€‹éšæ®µä¸€æ¨™æº–")
    print("=" * 60)
    
    # è¼‰å…¥è¡“èª
    print("\nğŸ“š æ­£åœ¨è¼‰å…¥è¡“èª...")
    terms = load_terms()
    print(f"âœ… è¼‰å…¥äº† {len(terms)} å€‹è¡“èª")
    print(f"   ç¯„ä¾‹ï¼š{', '.join(terms[:10])}")
    
    # æ”¶é›†æ‰€æœ‰å¦ä¾‹æª”æ¡ˆ
    case_files = list(CASES_DIR.rglob("*.md"))
    print(f"\nğŸ“‚ æ‰¾åˆ° {len(case_files)} å€‹å¦ä¾‹æª”æ¡ˆ")
    
    # æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†å‰ 5 å€‹
    if test_mode:
        case_files = case_files[:5]
        print(f"âš ï¸  æ¸¬è©¦æ¨¡å¼ï¼šåªè™•ç†å‰ {len(case_files)} å€‹æª”æ¡ˆ")
    
    # è™•ç†æª”æ¡ˆ
    print(f"\nğŸ”— é–‹å§‹å»ºç«‹é€£çµ...")
    if dry_run:
        print("âš ï¸  DRY RUN æ¨¡å¼ï¼šä¸æœƒå¯¦éš›ä¿®æ”¹æª”æ¡ˆ\n")
    
    modified_count = 0
    for i, filepath in enumerate(case_files, 1):
        if process_case_file(filepath, terms, dry_run):
            modified_count += 1
            print(f"  [{i}/{len(case_files)}] âœ… {filepath.name}")
        else:
            print(f"  [{i}/{len(case_files)}] â­ï¸  {filepath.name} (ç„¡éœ€ä¿®æ”¹)")
    
    # çµ±è¨ˆå ±å‘Š
    print("\n" + "=" * 60)
    print("âœ… åŸ·è¡Œå®Œæˆ")
    print(f"   ä¿®æ”¹æª”æ¡ˆæ•¸ï¼š{modified_count}/{len(case_files)}")
    print(f"   æœªä¿®æ”¹ï¼š{len(case_files) - modified_count}")
    
    if dry_run:
        print("\nğŸ’¡ é€™æ˜¯ DRY RUNï¼Œå¯¦éš›æª”æ¡ˆæœªè¢«ä¿®æ”¹")
        print("   è‹¥è¦åŸ·è¡Œå¯¦éš›ä¿®æ”¹ï¼Œè«‹ä½¿ç”¨ï¼špython link_knowledge.py --commit")
    else:
        print("\nğŸ’¾ å·²å„²å­˜æ‰€æœ‰è®Šæ›´")
        print("   å»ºè­°åŸ·è¡Œï¼šgit diff æŸ¥çœ‹è®Šæ›´")
    print("=" * 60)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='çŸ¥è­˜é»è‡ªå‹•é€£çµ (Tokenå„ªåŒ–ç‰ˆ)')
    parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ“¬åŸ·è¡Œï¼Œä¸å¯¦éš›ä¿®æ”¹æª”æ¡ˆ')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ï¼Œåªè™•ç†å‰5å€‹æª”æ¡ˆ')
    parser.add_argument('--commit', action='store_true', help='åŸ·è¡Œå¯¦éš›ä¿®æ”¹')
    
    args = parser.parse_args()
    
    # é è¨­ç‚º dry runï¼ˆå®‰å…¨èµ·è¦‹ï¼‰
    if args.commit:
        main(dry_run=False, test_mode=args.test)
    else:
        main(dry_run=True, test_mode=args.test)
