#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¡“èªç´¢å¼•å»ºç«‹å·¥å…· (Term Index Builder)
ç”¨é€”ï¼šæƒæ glossary/ å’Œ theory/ ç›®éŒ„ï¼Œç”Ÿæˆ term_index.json
ç›®æ¨™ï¼šé¿å… AI Agent é‡è¤‡æƒæç›®éŒ„ï¼Œç¯€çœ Token æ¶ˆè€—
"""

import json
from pathlib import Path
from typing import List, Dict

# è·¯å¾‘è¨­å®š
BASE_DIR = Path(__file__).resolve().parent.parent
GLOSSARY_DIR = BASE_DIR / "glossary"
THEORY_DIR = BASE_DIR / "theory"
OUTPUT_FILE = BASE_DIR / "data" / "term_index.json"


def scan_directory(directory: Path, category: str) -> List[Dict[str, str]]:
    """
    æƒææŒ‡å®šç›®éŒ„ï¼Œæå–æ‰€æœ‰ .md æª”æ¡ˆçš„è¡“èªè³‡è¨Š
    
    Args:
        directory: è¦æƒæçš„ç›®éŒ„
        category: é¡åˆ¥åç¨±ï¼ˆglossary æˆ– theoryï¼‰
    
    Returns:
        è¡“èªåˆ—è¡¨ï¼Œæ¯å€‹è¡“èªåŒ…å« name, category, file_path
    """
    terms = []
    
    if not directory.exists():
        print(f"âš ï¸  ç›®éŒ„ä¸å­˜åœ¨: {directory}")
        return terms
    
    for md_file in sorted(directory.glob("*.md")):
        # è·³é README
        if md_file.name.upper() == "README.MD":
            continue
        
        term_name = md_file.stem
        
        # è®€å–ç¬¬ä¸€è¡Œä½œç‚ºæè¿°ï¼ˆé¸ç”¨ï¼‰
        try:
            first_line = md_file.read_text(encoding='utf-8').split('\n')[0]
            description = first_line.replace('#', '').strip()
        except Exception as e:
            description = term_name
            print(f"âš ï¸  ç„¡æ³•è®€å– {md_file.name}: {e}")
        
        terms.append({
            "name": term_name,
            "category": category,
            "file_path": str(md_file.relative_to(BASE_DIR)),
            "description": description
        })
    
    return terms


def build_index() -> Dict:
    """
    å»ºç«‹å®Œæ•´çš„è¡“èªç´¢å¼•
    
    Returns:
        åŒ…å«æ‰€æœ‰è¡“èªçš„ç´¢å¼•å­—å…¸
    """
    print("ğŸ” é–‹å§‹å»ºç«‹è¡“èªç´¢å¼•...\n")
    
    # æƒæ glossary
    print(f"ğŸ“š æƒæ glossary/ ...")
    glossary_terms = scan_directory(GLOSSARY_DIR, "glossary")
    print(f"   âœ… æ‰¾åˆ° {len(glossary_terms)} å€‹è¡“èª\n")
    
    # æƒæ theory
    print(f"ğŸ“– æƒæ theory/ ...")
    theory_terms = scan_directory(THEORY_DIR, "theory")
    print(f"   âœ… æ‰¾åˆ° {len(theory_terms)} å€‹ç†è«–ä¸»é¡Œ\n")
    
    # å»ºç«‹ç´¢å¼•çµæ§‹
    index = {
        "metadata": {
            "total_terms": len(glossary_terms) + len(theory_terms),
            "glossary_count": len(glossary_terms),
            "theory_count": len(theory_terms),
            "generated_by": "build_term_index.py"
        },
        "glossary": glossary_terms,
        "theory": theory_terms,
        # æä¾›å¿«é€ŸæŸ¥è©¢çš„åç¨±åˆ—è¡¨
        "term_names": {
            "glossary": [t["name"] for t in glossary_terms],
            "theory": [t["name"] for t in theory_terms]
        }
    }
    
    return index


def save_index(index: Dict):
    """
    å„²å­˜ç´¢å¼•åˆ° JSON æª”æ¡ˆ
    
    Args:
        index: è¡“èªç´¢å¼•å­—å…¸
    """
    # ç¢ºä¿ data ç›®éŒ„å­˜åœ¨
    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç´¢å¼•å·²å„²å­˜: {OUTPUT_FILE}")
    print(f"   ç¸½è¡“èªæ•¸: {index['metadata']['total_terms']}")
    print(f"   - glossary: {index['metadata']['glossary_count']}")
    print(f"   - theory: {index['metadata']['theory_count']}")


def main():
    """ä¸»åŸ·è¡Œæµç¨‹"""
    print("=" * 60)
    print("è¡“èªç´¢å¼•å»ºç«‹å·¥å…·")
    print("=" * 60)
    print()
    
    # å»ºç«‹ç´¢å¼•
    index = build_index()
    
    # å„²å­˜æª”æ¡ˆ
    save_index(index)
    
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ’¡ ä½¿ç”¨èªªæ˜ï¼š")
    print("   AI Agent ç¾åœ¨å¯ä»¥è®€å– data/term_index.json")
    print("   è€Œä¸éœ€è¦æƒæ glossary/ å’Œ theory/ ç›®éŒ„")
    print("   é€™å°‡å¤§å¹…æ¸›å°‘ Token æ¶ˆè€—")


if __name__ == "__main__":
    main()
