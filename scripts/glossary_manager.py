#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¡“èªç®¡ç†å·¥å…·
ç®¡ç† glossary è³‡æ–™å¤¾ä¸­çš„è¡“èªæª”æ¡ˆ
âš¡ Token å„ªåŒ–ç‰ˆï¼šæ”¹ç”¨ guali_db.json è€Œéæƒææ‰€æœ‰ .md æª”æ¡ˆ
"""

import argparse
import yaml
import json
from pathlib import Path
from typing import List, Dict, Set
import re

BASE_DIR = Path(r"c:\Users\smallshieh\Obsidianç­†è¨˜")
GLOSSARY_DIR = BASE_DIR / "glossary"
CONFIG_DIR = BASE_DIR / "config"
DB_FILE = BASE_DIR / "data" / "guali_db.json"

class GlossaryManager:
    def __init__(self):
        self.glossary_dir = GLOSSARY_DIR
        self.config_file = CONFIG_DIR / "glossary_templates.yaml"
        self.templates = self.load_templates()
    
    def load_templates(self) -> Dict:
        """è¼‰å…¥è¡“èªæ¨¡æ¿"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {}
        return {}
    
    def check_missing(self, scan_dir: Path = None) -> Set[str]:
        """
        æª¢æŸ¥ç¼ºå¤±çš„è¡“èª
        âš¡ Token å„ªåŒ–ï¼šæ”¹ç”¨ guali_db.json è€Œéæƒææ‰€æœ‰ .md æª”æ¡ˆ
        """
        # æ”¶é›†æ‰€æœ‰ [[glossary/XXX]] é€£çµ
        referenced_terms = set()
        
        # å„ªå…ˆå¾ guali_db.json è®€å–ï¼ˆToken å„ªåŒ–ï¼‰
        if DB_FILE.exists():
            print("ğŸ“– å¾ guali_db.json è®€å–è¡“èªå¼•ç”¨ï¼ˆToken å„ªåŒ–æ¨¡å¼ï¼‰...")
            try:
                with open(DB_FILE, 'r', encoding='utf-8') as f:
                    db = json.load(f)
                
                for case in db:
                    content = case.get('content', '')
                    # åŒ¹é… [[glossary/XXX]]
                    matches = re.findall(r'\[\[glossary/([^\]]+)\]\]', content)
                    # è™•ç†åˆ¥å [[glossary/Term|Alias]] -> Term
                    for m in matches:
                        term = m.split('|')[0]
                        referenced_terms.add(term)
                
                print(f"   âœ… å¾è³‡æ–™åº«æ‰¾åˆ° {len(referenced_terms)} å€‹è¡“èªå¼•ç”¨")
            except Exception as e:
                print(f"   âš ï¸  è®€å– guali_db.json å¤±æ•—: {e}")
                print("   â„¹ï¸  é€€å›åˆ°æƒææ¨¡å¼...")
                # è‹¥ JSON è®€å–å¤±æ•—ï¼Œæ‰é€€å›æƒææ¨¡å¼
                if scan_dir is None:
                    scan_dir = BASE_DIR
                for md_file in scan_dir.rglob("*.md"):
                    try:
                        content = md_file.read_text(encoding='utf-8')
                        matches = re.findall(r'\[\[glossary/([^\]]+)\]\]', content)
                        for m in matches:
                            term = m.split('|')[0]
                            referenced_terms.add(term)
                    except:
                        continue
        else:
            print(f"âš ï¸  æ‰¾ä¸åˆ° guali_db.jsonï¼Œä½¿ç”¨æƒææ¨¡å¼...")
            if scan_dir is None:
                scan_dir = BASE_DIR
            for md_file in scan_dir.rglob("*.md"):
                try:
                    content = md_file.read_text(encoding='utf-8')
                    matches = re.findall(r'\[\[glossary/([^\]]+)\]\]', content)
                    for m in matches:
                        term = m.split('|')[0]
                        referenced_terms.add(term)
                except:
                    continue
        
        # æª¢æŸ¥å“ªäº›è¡“èªæª”æ¡ˆä¸å­˜åœ¨
        existing_terms = set()
        for term_file in self.glossary_dir.glob("*.md"):
            existing_terms.add(term_file.stem)
        
        missing_terms = referenced_terms - existing_terms
        
        print(f"\nğŸ“Š è¡“èªæª¢æŸ¥çµæœ")
        print("=" * 60)
        print(f"å¼•ç”¨çš„è¡“èª: {len(referenced_terms)}")
        print(f"å·²å­˜åœ¨è¡“èª: {len(existing_terms)}")
        print(f"ç¼ºå¤±è¡“èª: {len(missing_terms)}")
        
        if missing_terms:
            print(f"\nâŒ ç¼ºå¤±çš„è¡“èª:")
            for term in sorted(missing_terms):
                print(f"   - {term}")
        else:
            print(f"\nâœ… æ‰€æœ‰è¡“èªéƒ½å·²å­˜åœ¨")
        
        return missing_terms
    
    def create_term(self, term_name: str, template: str = "basic", dry_run: bool = False):
        """å‰µå»ºè¡“èªæª”æ¡ˆ"""
        term_file = self.glossary_dir / f"{term_name}.md"
        
        if term_file.exists():
            print(f"âŒ è¡“èªå·²å­˜åœ¨: {term_name}")
            return False
        
        # ç²å–æ¨¡æ¿
        template_content = self.templates.get(template, self.templates.get("basic", ""))
        if not template_content:
            template_content = f"# {term_name}\n\n## å®šç¾©\n\n[å¾…è£œå……]\n"
        else:
            template_content = template_content.replace("{{term}}", term_name)
        
        print(f"\nğŸ“ å‰µå»ºè¡“èª: {term_name}")
        print(f"   æ¨¡æ¿: {template}")
        
        if dry_run:
            print("   âš ï¸  DRY RUN - ä¸å¯¦éš›å‰µå»º")
            print(f"\nå…§å®¹é è¦½:\n{template_content[:200]}...")
            return True
        
        term_file.write_text(template_content, encoding='utf-8')
        print(f"   âœ… å·²å‰µå»º: {term_file}")
        
        return True
    
    def batch_create(self, terms: List[str], template: str = "basic", dry_run: bool = False):
        """æ‰¹æ¬¡å‰µå»ºè¡“èª"""
        print(f"\nğŸ“¦ æ‰¹æ¬¡å‰µå»º {len(terms)} å€‹è¡“èª")
        
        created = 0
        skipped = 0
        
        for term in terms:
            if self.create_term(term, template, dry_run):
                created += 1
            else:
                skipped += 1
        
        print(f"\nâœ… å‰µå»º: {created}, â­ï¸  è·³é: {skipped}")
    
    def build_index(self, output_file: str = "glossary/README.md"):
        """å»ºç«‹è¡“èªç´¢å¼•"""
        terms = []
        for term_file in sorted(self.glossary_dir.glob("*.md")):
            if term_file.name == "README.md":
                continue
            
            term_name = term_file.stem
            # è®€å–ç¬¬ä¸€è¡Œä½œç‚ºç°¡çŸ­æè¿°
            try:
                first_line = term_file.read_text(encoding='utf-8').split('\n')[0]
                desc = first_line.replace('#', '').strip()
            except:
                desc = term_name
            
            terms.append({
                'name': term_name,
                'desc': desc,
                'file': term_file.name
            })
        
        # ç”Ÿæˆç´¢å¼•
        index_content = "# è¡“èªç´¢å¼•\n\n"
        index_content += f"ç¸½è¨ˆ: {len(terms)} å€‹è¡“èª\n\n"
        
        # æŒ‰é¦–å­—æ¯åˆ†çµ„
        groups = {}
        for term in terms:
            first_char = term['name'][0]
            if first_char not in groups:
                groups[first_char] = []
            groups[first_char].append(term)
        
        for char in sorted(groups.keys()):
            index_content += f"\n## {char}\n\n"
            for term in sorted(groups[char], key=lambda x: x['name']):
                index_content += f"- [[{term['name']}]] - {term['desc']}\n"
        
        index_path = BASE_DIR / output_file
        index_path.write_text(index_content, encoding='utf-8')
        
        print(f"\nâœ… ç´¢å¼•å·²å»ºç«‹: {index_path}")
        print(f"   è¡“èªæ•¸: {len(terms)}")

def main():
    parser = argparse.ArgumentParser(description='è¡“èªç®¡ç†å·¥å…· (Token å„ªåŒ–ç‰ˆ)')
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤')
    
    # check-missing å‘½ä»¤
    check_parser = subparsers.add_parser('check-missing', help='æª¢æŸ¥ç¼ºå¤±è¡“èª')
    
    # create å‘½ä»¤
    create_parser = subparsers.add_parser('create', help='å‰µå»ºè¡“èª')
    create_parser.add_argument('term', help='è¡“èªåç¨±')
    create_parser.add_argument('--template', default='basic', help='æ¨¡æ¿é¡å‹')
    create_parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ“¬åŸ·è¡Œ')
    
    # batch-create å‘½ä»¤
    batch_parser = subparsers.add_parser('batch-create', help='æ‰¹æ¬¡å‰µå»ºè¡“èª')
    batch_parser.add_argument('--template', default='basic', help='æ¨¡æ¿é¡å‹')
    batch_parser.add_argument('--dry-run', action='store_true', help='æ¨¡æ“¬åŸ·è¡Œ')
    
    # build-index å‘½ä»¤
    index_parser = subparsers.add_parser('build-index', help='å»ºç«‹è¡“èªç´¢å¼•')
    index_parser.add_argument('--output', default='glossary/README.md', help='è¼¸å‡ºæª”æ¡ˆ')
    
    args = parser.parse_args()
    
    manager = GlossaryManager()
    
    if args.command == 'check-missing':
        missing = manager.check_missing()
        if missing:
            print(f"\nğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰¹æ¬¡å‰µå»º:")
            print(f"   python scripts/glossary_manager.py batch-create")
    
    elif args.command == 'create':
        manager.create_term(args.term, args.template, args.dry_run)
    
    elif args.command == 'batch-create':
        missing = manager.check_missing()
        if missing:
            manager.batch_create(list(missing), args.template, args.dry_run)
        else:
            print("âœ… ç„¡ç¼ºå¤±è¡“èª")
    
    elif args.command == 'build-index':
        manager.build_index(args.output)
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
